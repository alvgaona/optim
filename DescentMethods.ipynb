{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfb6214-938c-46fd-8bbf-8bb2d9d2e847",
   "metadata": {},
   "source": [
    "# Descent Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "625d9901-9a9a-473e-a643-13f0398a568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff: gradient\n",
    "using LinearAlgebra: norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e848688-8fd9-4349-b403-6c343ff2b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "function backtracking(f, x, Δx; t₀=1, β=0.5, α=0.25)\n",
    "  \"\"\"\n",
    "  Performs backtracking line search to find a suitable step length.\n",
    "\n",
    "  Args:\n",
    "    f (Function): the objective function\n",
    "    x (Vector): the current point\n",
    "    t₀ (Float64): initial step length\n",
    "    Δx (Vector): the search direction\n",
    "    β (Float64): backtracking factor. β ∈ (0,1)\n",
    "    α (Float64): sufficient decrease condition parameter. α ∈ (0, 0.5)\n",
    "  \"\"\"\n",
    "\n",
    "  t = t₀\n",
    "  fₓ = f(x)\n",
    "\n",
    "  while f(x + t * Δx) > fₓ + α * t * dot(gradient(f, x), Δx)\n",
    "    t *= β\n",
    "  end\n",
    "\n",
    "  return t\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb9b0b-dfb4-4394-8796-af6f474e0866",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784c7cef-27a0-47f3-bc4c-bb1a68bce33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient_descent(f, x0; tol=1e-8, max_iters=1000)\n",
    "  \"\"\"\n",
    "  Performs the gradient descent algorithm with backtracking line search\n",
    "\n",
    "  Args:\n",
    "    f (Function): the objective function\n",
    "    x (Vector): the initial point\n",
    "    tol (Float64): tolerance to break the loop\n",
    "    max_iters (Int64): maximum number of iterations\n",
    "  \"\"\"\n",
    "  x = copy(x0)\n",
    "  f_val = f(x)\n",
    "\n",
    "  iteration = 0\n",
    "\n",
    "  for i = 1:max_iters\n",
    "    iteration = i\n",
    "    ∇f = gradient(f, x)\n",
    "\n",
    "    if norm(∇f) < tol\n",
    "      break\n",
    "    end\n",
    "\n",
    "    t = backtracking(f, x, -∇f)\n",
    "\n",
    "    x = x - t * ∇f\n",
    "    f_val = f(x)\n",
    "  end\n",
    "\n",
    "  return x, f_val, iteration\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9531d0f-11a0-4422-a8d7-712acd8a78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.071095 seconds (22.19 k allocations: 1.438 MiB, 99.68% compilation time: 93% of which was recompilation)\n",
      "x_min = [0.5] at f(x_min)=2.5592666966582254 (13 iterations)\n"
     ]
    }
   ],
   "source": [
    "f(x) = x[1]^2 - x[1]\n",
    "x₀ = [1000]\n",
    "\n",
    "x, f_min, iterations = @time gradient_descent(f, x₀, max_iters=1000)\n",
    "println(\"x_min = $x at f(x_min)=$f_val ($iter iterations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dccd7624-9a8e-4beb-aa33-2c4acad6a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.149230 seconds (19.39 k allocations: 1.248 MiB, 99.91% compilation time: 97% of which was recompilation)\n",
      "x_min = [0.0, 0.0] at f(x_min)=-1.0 (13 iterations)\n"
     ]
    }
   ],
   "source": [
    "# 2-dimensional function\n",
    "g(x) = x[1]^2 + x[2]^2 - 1\n",
    "x₀ = [1.0, 1.0]\n",
    "\n",
    "x, g_val, iterations = @time gradient_descent(g, x₀, max_iters=1000)\n",
    "println(\"x_min = $x at f(x_min)=$g_val ($iter iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb3063-1868-455f-a5e2-c58e96bec17d",
   "metadata": {},
   "source": [
    "## Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb8a28e-3f1d-4b3e-8190-08918caff320",
   "metadata": {},
   "outputs": [],
   "source": [
    "function steepest_descent(f, x₀, Δxsd, norm; tol=1e-12, max_iters=1000)\n",
    "  x = copy(x₀)\n",
    "  f_val = f(x)\n",
    "\n",
    "  iter = 0\n",
    "  for i = 1:max_iters\n",
    "    iter = i\n",
    "    ∇f = gradient(f, x)\n",
    "\n",
    "    if norm(∇f) < tol\n",
    "      break\n",
    "    end\n",
    "\n",
    "    Δx = Δxsd(f, x)\n",
    "\n",
    "    t = backtracking(f, x, Δx, α=0.1, β=0.7)\n",
    "    x = x + t * Δx\n",
    "    f_val = f(x)\n",
    "  end\n",
    "\n",
    "  return x, f_val, iter\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5d737c5-7bed-45d5-a085-81069e7cff5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_min = [-0.346573502949323, 0.0] at f(x_min)=2.5592666966582254 (13 iterations)\n"
     ]
    }
   ],
   "source": [
    "f(x) = exp(x[1] + 3 * x[2] - 0.1) + exp(x[1] - 3 * x[2] - 0.1) + exp(-x[1] - 0.1)\n",
    "P1 = [2 0; 0 8]\n",
    "# P2 = [8 0; 0 2]\n",
    "norm_fn(z) = z' * P1 * z\n",
    "Δxsd(f, x) = -inv(P1) * gradient(f, x)\n",
    "\n",
    "x, f_val, iter = steepest_descent(f, [0, 0], Δxsd, norm_fn)\n",
    "\n",
    "println(\"x_min = $x at f(x_min)=$f_val ($iter iterations)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
