{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfb6214-938c-46fd-8bbf-8bb2d9d2e847",
   "metadata": {},
   "source": [
    "# Descent Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "625d9901-9a9a-473e-a643-13f0398a568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LaTeXStrings\n",
    "using Plots\n",
    "using ForwardDiff: gradient\n",
    "using LinearAlgebra: norm, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7328f62f-928b-4909-b267-83750e8a4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "function bisect_min(f; a=-1e12, b=1e12, tol=1e-6, max_iters=1000)\n",
    "  \"\"\"\n",
    "  Find the minimum of a one-dimensional function f(x) using binary search.\n",
    "\n",
    "  Args:\n",
    "      f (Function): The function to minimize.\n",
    "      a (Float64): The left bound of the search interval.\n",
    "      b (Float64): The right bound of the search interval.\n",
    "      tol (Float64): The desired tolerance for the minimum.\n",
    "      max_iters (Int64): The maximum number of iterations possible.\n",
    "            \n",
    "  Returns:\n",
    "      Float64: The minimum of the function f(x) within the given interval.\n",
    "  \"\"\"\n",
    "  k = 1\n",
    "  a₁ = a\n",
    "  b₁ = b\n",
    "\n",
    "  while k < max_iters\n",
    "    ∇f = gradient(f, [(a₁ + b₁) / 2])[1]\n",
    "    \n",
    "    if b₁ - a₁ < tol\n",
    "      break\n",
    "    end\n",
    "\n",
    "    if ∇f > 0\n",
    "      b₁ = (a₁ + b₁) / 2\n",
    "    else\n",
    "      a₁ = (a₁ + b₁) / 2\n",
    "    end\n",
    "\n",
    "    k += 1\n",
    "  end\n",
    "  \n",
    "  return (a₁ + b₁) / 2\n",
    "end;\n",
    "\n",
    "function exact(f, x, Δx; tol=1e-6, max_iters=100000)\n",
    "    \"\"\"\n",
    "    Performs exact line search to find the optimal step size.\n",
    "\n",
    "    Parameters:\n",
    "    f (function): The objective function.\n",
    "    x (Array{Float64,1}): The current point.\n",
    "    Δx (Array{Float64,1}): The search direction.\n",
    "    tol (Float64): The tolerance for the termination condition.\n",
    "\n",
    "    Returns:\n",
    "    Float64: The optimal step size.\n",
    "    \"\"\"\n",
    "    h(s) = x .+ s[1] * Δx \n",
    "    return bisect_min(f ∘ h)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e848688-8fd9-4349-b403-6c343ff2b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "function backtracking(f, x, Δx; t₀=1, β=0.5, α=0.25)\n",
    "  \"\"\"\n",
    "  Performs backtracking line search to find a suitable step length.\n",
    "\n",
    "  Args:\n",
    "    f (Function): the objective function\n",
    "    x (Vector): the current point\n",
    "    t₀ (Float64): initial step length\n",
    "    Δx (Vector): the search direction\n",
    "    β (Float64): backtracking factor. β ∈ (0,1)\n",
    "    α (Float64): sufficient decrease condition parameter. α ∈ (0, 0.5)\n",
    "  \"\"\"\n",
    "\n",
    "  t = t₀\n",
    "  fₓ = f(x)\n",
    "\n",
    "  while f(x + t * Δx) > fₓ + α * t * dot(gradient(f, x), Δx)\n",
    "    t *= β\n",
    "  end\n",
    "\n",
    "  return t\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb9b0b-dfb4-4394-8796-af6f474e0866",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "784c7cef-27a0-47f3-bc4c-bb1a68bce33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient_descent(f, x₀; tol=1e-8, max_iters=1000, linesearch=:backtracking)\n",
    "  \"\"\"\n",
    "  Performs the gradient descent algorithm with backtracking line search\n",
    "\n",
    "  Args:\n",
    "    f (Function): the objective function\n",
    "    x (Vector): the initial point\n",
    "    tol (Float64): tolerance to break the loop\n",
    "    max_iters (Int64): maximum number of iterations\n",
    "  \"\"\"\n",
    "  x = copy(x₀)\n",
    "  f_val = f(x)\n",
    "\n",
    "  iteration = 0\n",
    "\n",
    "  for i = 1:max_iters\n",
    "    iteration = i\n",
    "    ∇f = gradient(f, x)\n",
    "\n",
    "    if norm(∇f) < tol\n",
    "      break\n",
    "    end\n",
    "\n",
    "    if linesearch == :exact\n",
    "      t = exact(f, x, -∇f)\n",
    "    else\n",
    "      t = backtracking(f, x, -∇f, α=0.1, β=0.7)\n",
    "    end\n",
    "\n",
    "    x = x - t * ∇f\n",
    "    f_val = f(x)\n",
    "  end\n",
    "\n",
    "  return x, f_val, iteration\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e6433208-cb95-4167-bd5e-1dd9f99df6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.869742 seconds (1.67 M allocations: 114.637 MiB, 1.88% gc time, 99.87% compilation time: 2% of which was recompilation)\n",
      "x_min = [3.602879701896376e-11] at f(x_min)=1.298074214633692e-21 (29 iterations - backtracking)\n",
      "  0.292914 seconds (809.86 k allocations: 55.930 MiB, 2.87% gc time, 99.77% compilation time: 3% of which was recompilation)\n",
      "x_min = [9.578043450053121e-13] at f(x_min)=9.17389163311055e-25 (3 iterations - exact)\n"
     ]
    }
   ],
   "source": [
    "f(x) = x[1]^2\n",
    "x₀ = [5]\n",
    "\n",
    "x, f_min, iter = @time gradient_descent(f, x₀, tol=1e-10, max_iters=1000)\n",
    "println(\"x_min = $x at f(x_min)=$f_min ($iter iterations - backtracking)\")\n",
    "\n",
    "x, f_min, iter = @time gradient_descent(f, x₀, tol=1e-10, max_iters=1000, linesearch=:exact)\n",
    "println(\"x_min = $x at f(x_min)=$f_min ($iter iterations - exact)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dccd7624-9a8e-4beb-aa33-2c4acad6a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.140520 seconds (123.83 k allocations: 6.243 MiB, 93.21% compilation time: 94% of which was recompilation)\n",
      "x_min = [-0.34657359027997275, 7.852103101148952e-9] at f(x_min)=2.5592666966582156 (1000 iterations - backtracking)\n",
      "  0.030946 seconds (34.80 k allocations: 2.150 MiB, 91.69% compilation time: 100% of which was recompilation)\n",
      "x_min = [-0.3465735902696454, 7.239816783138962e-12] at f(x_min)=2.5592666966582156 (27 iterations - exact)\n"
     ]
    }
   ],
   "source": [
    "# 2-dimensional function\n",
    "g(x) = exp(x[1] + 3 * x[2] - 0.1) + exp(x[1] - 3 * x[2] - 0.1) + exp(-x[1] - 0.1)\n",
    "x₀ = [0, 1]\n",
    "\n",
    "x, g_min, iter = @time gradient_descent(g, x₀, tol=1e-10, max_iters=1000)\n",
    "println(\"x_min = $x at f(x_min)=$g_min ($iter iterations - backtracking)\")\n",
    "\n",
    "x, g_min, iter = @time gradient_descent(g, x₀, tol=1e-10, max_iters=1000, linesearch=:exact)\n",
    "println(\"x_min = $x at f(x_min)=$g_min ($iter iterations - exact)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb3063-1868-455f-a5e2-c58e96bec17d",
   "metadata": {},
   "source": [
    "## Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6cb8a28e-3f1d-4b3e-8190-08918caff320",
   "metadata": {},
   "outputs": [],
   "source": [
    "function steepest_descent(f, x₀, Δxsd, norm; tol=1e-4, max_iters=1000, linesearch=:backtracking)\n",
    "  x = copy(x₀)\n",
    "  f_val = f(x)\n",
    "\n",
    "  iter = 0\n",
    "  for i = 1:max_iters\n",
    "    iter = i\n",
    "    Δx = Δxsd(f, x)\n",
    "\n",
    "    if norm(Δx) < tol\n",
    "      break\n",
    "    end\n",
    "\n",
    "    if linesearch == :exact\n",
    "      t = exact(f, x, Δx)\n",
    "    else\n",
    "      t = backtracking(f, x, Δx, α=0.1, β=0.7)\n",
    "    end\n",
    "\n",
    "    x += t * Δx\n",
    "    f_val = f(x)\n",
    "  end\n",
    "\n",
    "  return x, f_val, iter\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8f6fdf79-ce75-45ba-9cdb-508a36e4d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = exp(x[1] + 3 * x[2] - 0.1) + exp(x[1] - 3 * x[2] - 0.1) + exp(-x[1] - 0.1)\n",
    "P1 = [2 0; 0 8]\n",
    "# P2 = [8 0; 0 2]\n",
    "norm_fn(z) = z' * P1 * z\n",
    "Δxsd(f, x) = -inv(P1) * gradient(f, x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1775339d-4edd-48e8-a5f9-b110b307e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.103401 seconds (30.93 k allocations: 1.991 MiB, 99.49% compilation time: 80% of which was recompilation)\n",
      "x_min = [-0.3465735859500477, -1.5162050377715008e-6] at f(x_min)=2.5592666966714535 (22 iterations - backtracking)\n"
     ]
    }
   ],
   "source": [
    "x, f_val, iter = @time steepest_descent(f, [-5, 5], Δxsd, norm_fn, tol=1e-10, linesearch=:backtracking)\n",
    "\n",
    "println(\"x_min = $x at f(x_min)=$f_val ($iter iterations - backtracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "393f8af3-8eec-447d-af29-e2abbfc94034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.197424 seconds (25.15 k allocations: 1.628 MiB, 53.29% compilation time: 100% of which was recompilation)\n",
      "x_min = [-0.3465731907770624, -1.1983563574664841e-7] at f(x_min)=2.5592666966585025 (10 iterations - exact line search)\n"
     ]
    }
   ],
   "source": [
    "x, f_val, iter = @time steepest_descent(f, [-5, 5], Δxsd, norm_fn, tol=1e-10, linesearch=:exact)\n",
    "\n",
    "println(\"x_min = $x at f(x_min)=$f_val ($iter iterations - exact line search)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
